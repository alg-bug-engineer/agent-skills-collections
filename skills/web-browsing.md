# Web Browsing Skills

Web æµè§ˆæŠ€èƒ½æ˜¯ Agent çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œå…è®¸æ™ºèƒ½ä½“è®¿é—®äº’è”ç½‘ä¿¡æ¯ã€æœç´¢æ•°æ®ã€æµè§ˆç½‘é¡µç­‰ã€‚

## ğŸ“– æ¦‚è¿°

Web æµè§ˆæŠ€èƒ½ä½¿ AI Agent èƒ½å¤Ÿï¼š
- æœç´¢äº’è”ç½‘ä¿¡æ¯
- è®¿é—®å’Œè§£æç½‘é¡µå†…å®¹
- ä»ç½‘ç«™æå–æ•°æ®
- éµå¾ªé“¾æ¥è¿›è¡Œå¯¼èˆª
- å¤„ç†åŠ¨æ€ç½‘é¡µ

## ğŸ› ï¸ ä¸»è¦å·¥å…·å’Œåº“

### 1. LangChain Web Browsing Tools

#### DuckDuckGo Search
```python
from langchain_community.tools import DuckDuckGoSearchRun

search = DuckDuckGoSearchRun()

# åŸºç¡€æœç´¢
results = search.run("Python programming tutorials")
print(results)

# ä½œä¸ºå·¥å…·ä½¿ç”¨
from langchain.tools import Tool
tool = Tool(
    name="DuckDuckGo Search",
    func=search.run,
    description="Search the web using DuckDuckGo"
)
```

#### Google Searchï¼ˆéœ€è¦ API Keyï¼‰
```python
from langchain_community.tools import GoogleSearchAPIWrapper

search = GoogleSearchAPIWrapper(
    google_api_key="your-api-key",
    google_cse_id="your-cse-id"
)

results = search.results("AI agents", num_results=5)
```

#### Bing Searchï¼ˆéœ€è¦ API Keyï¼‰
```python
from langchain_community.utilities import BingSearchAPIWrapper

search = BingSearchAPIWrapper(
    bing_subscription_key="your-key",
    bing_search_url="https://api.bing.microsoft.com/v7.0/search"
)

results = search.run("machine learning")
```

### 2. AutoGen Web Browsing

```python
from autogen import AssistantAgent, UserProxyAgent
import requests

# å®šä¹‰ Web æµè§ˆå·¥å…·
def browse_web(url: str) -> str:
    """Browse a webpage and return its content"""
    try:
        response = requests.get(url)
        return response.text[:1000]
    except Exception as e:
        return f"Error: {str(e)}"

# åˆ›å»ºä½¿ç”¨å·¥å…·çš„æ™ºèƒ½ä½“
assistant = AssistantAgent(
    name="assistant",
    llm_config={
        "config_list": config_list,
        "functions": [
            {
                "name": "browse_web",
                "description": "Browse a webpage and return its content",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "The URL to browse"
                        }
                    },
                    "required": ["url"]
                }
            }
        ]
    }
)
```

### 3. CrewAI Web Browsing

```python
from crewai import Agent, Task, Crew
from crewai.tools import SerperDevTool

# åˆ›å»ºæœç´¢å·¥å…·
search_tool = SerperDevTool()

# åˆ›å»ºå…·å¤‡ Web æµè§ˆèƒ½åŠ›çš„ Agent
researcher = Agent(
    role="Researcher",
    goal="Find information on the web",
    backstory="You are an expert researcher",
    tools=[search_tool],
    llm="gpt-4"
)

# åˆ›å»ºä»»åŠ¡
task = Task(
    description="Research the latest developments in AI agents",
    agent=researcher
)

# æ‰§è¡Œ
crew = Crew(agents=[researcher], tasks=[task])
result = crew.kickoff()
```

## ğŸ¯ é«˜çº§ Web æµè§ˆæŠ€èƒ½

### 1. ç½‘é¡µå†…å®¹æå–

#### ä½¿ç”¨ BeautifulSoup
```python
from bs4 import BeautifulSoup
import requests

def extract_text(url: str) -> str:
    """Extract main text from a webpage"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
    for script in soup(["script", "style"]):
        script.decompose()
    
    # è·å–æ–‡æœ¬
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    return text
```

#### ä½¿ç”¨ Trafilatura
```python
import trafilatura

def extract_article(url: str) -> str:
    """Extract article content from a webpage"""
    downloaded = trafilatura.fetch_url(url)
    result = trafilatura.extract(downloaded)
    return result
```

### 2. å¤šæ­¥æœç´¢å’Œæµè§ˆ

```python
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)

# åŠ è½½å¤šä¸ªæœç´¢å·¥å…·
tools = load_tools(
    ["serpapi", "requests_all"],
    serpapi_api_key="your-api-key"
)

# åˆ›å»º Agent
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# æ‰§è¡Œå¤æ‚æœç´¢
result = agent.run("""
1. Search for recent news about AI agents
2. Find specific articles about LangChain
3. Summarize the key points
""")
```

### 3. æ·±åº¦ç½‘é¡µçˆ¬å–

```python
import scrapy
from scrapy.crawler import CrawlerProcess

class WebSpider(scrapy.Spider):
    name = 'web_spider'
    
    def __init__(self, start_url=None, *args, **kwargs):
        super(WebSpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url] if start_url else []
    
    def parse(self, response):
        # æå–å½“å‰é¡µé¢å†…å®¹
        yield {
            'url': response.url,
            'title': response.css('title::text').get(),
            'content': response.css('body::text').get()
        }
        
        # è·Ÿè¸ªé“¾æ¥
        for link in response.css('a::attr(href)'):
            yield response.follow(link, self.parse)

# è¿è¡Œçˆ¬è™«
process = CrawlerProcess()
process.crawl(WebSpider, start_url='https://example.com')
process.start()
```

### 4. API é›†æˆ

```python
import requests

def fetch_api_data(endpoint: str, params: dict = None) -> dict:
    """Fetch data from a REST API"""
    response = requests.get(endpoint, params=params)
    if response.status_code == 200:
        return response.json()
    return {"error": f"Failed with status {response.status_code}"}

# ä½¿ç”¨ç¤ºä¾‹
data = fetch_api_data(
    "https://api.github.com/repos/langchain-ai/langchain",
    params={"state": "open"}
)
```

## ğŸ” æœç´¢ç­–ç•¥

### 1. å¤šå¼•æ“æœç´¢
```python
def multi_engine_search(query: str, engines: list) -> list:
    """Search using multiple search engines"""
    results = []
    for engine in engines:
        if engine == "duckduckgo":
            tool = DuckDuckGoSearchRun()
        elif engine == "google":
            tool = GoogleSearchAPIWrapper()
        elif engine == "bing":
            tool = BingSearchAPIWrapper()
        
        result = tool.run(query)
        results.append({
            "engine": engine,
            "results": result
        })
    
    return results

# ä½¿ç”¨
results = multi_engine_search(
    "AI agent frameworks",
    ["duckduckgo", "google", "bing"]
)
```

### 2. é€’å½’æœç´¢
```python
def recursive_search(query: str, depth: int = 2, current_depth: int = 0) -> list:
    """Recursively search and explore results"""
    if current_depth >= depth:
        return []
    
    # æœç´¢
    search = DuckDuckGoSearchRun()
    initial_results = search.run(query)
    
    # è§£æç»“æœå¹¶é€’å½’
    results = [{"query": query, "content": initial_results}]
    
    # å¦‚æœæœ‰é“¾æ¥ï¼Œè¿›ä¸€æ­¥æœç´¢
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦è§£æURL
    if current_depth < depth - 1:
        sub_results = recursive_search(
            f"more info about {query}",
            depth,
            current_depth + 1
        )
        results.extend(sub_results)
    
    return results
```

## ğŸ›¡ï¸ å®‰å…¨è€ƒè™‘

### 1. è¯·æ±‚é™åˆ¶
```python
import time
from functools import wraps

def rate_limit(max_calls: int, time_frame: int):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            
            # æ¸…ç†è¿‡æœŸè°ƒç”¨
            calls[:] = [c for c in calls if c > now - time_frame]
            
            if len(calls) >= max_calls:
                wait_time = time_frame - (now - calls[0])
                time.sleep(wait_time)
            
            calls.append(now)
            return func(*args, **kwargs)
        
        return wrapper
    return decorator

@rate_limit(max_calls=10, time_frame=60)
def browse_url(url: str):
    """Browse URL with rate limiting"""
    return requests.get(url)
```

### 2. éªŒè¯å’Œæ¸…ç†
```python
from urllib.parse import urlparse

def is_safe_url(url: str) -> bool:
    """Check if a URL is safe to visit"""
    parsed = urlparse(url)
    
    # åªå…è®¸ http å’Œ https
    if parsed.scheme not in ['http', 'https']:
        return False
    
    # é˜²æ­¢å†…éƒ¨ç½‘ç»œè®¿é—®
    if parsed.hostname in ['localhost', '127.0.0.1']:
        return False
    
    return True

def safe_browse(url: str):
    """Safely browse a URL"""
    if not is_safe_url(url):
        raise ValueError(f"Unsafe URL: {url}")
    
    response = requests.get(url)
    return response.text
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘è¯·æ±‚
```python
import concurrent.futures
import requests

def parallel_fetch(urls: list, max_workers: int = 5) -> list:
    """Fetch multiple URLs in parallel"""
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(requests.get, url): url
            for url in urls
        }
        
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                results.append({
                    "url": url,
                    "status": result.status_code,
                    "content": result.text[:500]
                })
            except Exception as e:
                results.append({
                    "url": url,
                    "error": str(e)
                })
    
    return results
```

### 2. ç¼“å­˜ç»“æœ
```python
import hashlib
import json
from pathlib import Path

class Cache:
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def _get_cache_path(self, key: str) -> Path:
        """Get cache file path for a key"""
        hashed = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{hashed}.json"
    
    def get(self, key: str):
        """Get cached value"""
        cache_path = self._get_cache_path(key)
        if cache_path.exists():
            with open(cache_path, 'r') as f:
                return json.load(f)
        return None
    
    def set(self, key: str, value):
        """Set cached value"""
        cache_path = self._get_cache_path(key)
        with open(cache_path, 'w') as f:
            json.dump(value, f)

# ä½¿ç”¨ç¤ºä¾‹
cache = Cache()

def cached_search(query: str):
    """Search with caching"""
    cached = cache.get(query)
    if cached:
        return cached
    
    search = DuckDuckGoSearchRun()
    result = search.run(query)
    
    cache.set(query, result)
    return result
```

## ğŸ“ æœ€ä½³å®è·µ

1. **ä½¿ç”¨å¤šä¸ªæœç´¢æº**: ä¸è¦ä¾èµ–å•ä¸€æœç´¢å¼•æ“
2. **å®ç°ç¼“å­˜**: é¿å…é‡å¤è¯·æ±‚ç›¸åŒå†…å®¹
3. **é™åˆ¶é€’å½’æ·±åº¦**: é˜²æ­¢æ— é™å¾ªç¯
4. **éªŒè¯ URL**: ç¡®ä¿åªè®¿é—®å®‰å…¨ç½‘ç«™
5. **éµå®ˆ robots.txt**: å°Šé‡ç½‘ç«™çš„çˆ¬è™«è§„åˆ™
6. **å¤„ç†é”™è¯¯**: å®ç°å¥å£®çš„é”™è¯¯å¤„ç†
7. **æ§åˆ¶è¯·æ±‚é¢‘ç‡**: é¿å…è¢«ç½‘ç«™å°ç¦
8. **è§£æç»“æ„åŒ–æ•°æ®**: ä½¿ç”¨ä¸“é—¨çš„è§£æå·¥å…·æé«˜å‡†ç¡®æ€§

## ğŸ”— ç›¸å…³èµ„æº

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Trafilatura](https://github.com/adbar/trafilatura)
- [Scrapy](https://scrapy.org/)
- [Requests Library](https://requests.readthedocs.io/)

---

**æ›´æ–°æ—¥æœŸ**: 2026-01-04
**ç‰ˆæœ¬**: v0.1
